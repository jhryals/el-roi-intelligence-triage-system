# EL ROI: AI-Driven Intelligence Triage System

**EL ROI** (Enhanced Language-based Rapid Open-source Intelligence) is an AI-powered triage system designed to support the early phases of the Intelligence Cycleâ€”**Collection**, **Processing & Exploitation**, and **Analysis & Production**.

It ingests live-streaming multilingual OSINT data, processes and enriches it through automated NLP pipelines, and intelligently surfaces thematically relevant insights. Analysts can query the system in natural language, explore communication clusters visually, and export structured intelligence summaries with source traceability.

The purpose of EL ROI is to accelerate insight discovery, reduce cognitive overload, and lay the groundwork for timely, actionable intelligence.

---

## Key Features

### 1. Collection
- Real-time ingestion of multilingual OSINT data (news, blogs, social platforms, etc.)
- Modular architecture designed to support future ingestion of multi-INT sources

### 2. Processing & Exploitation
- Language detection and translation (auto-handling mixed-language inputs)
- Named Entity Recognition (NER) and entity linking
- Text deduplication and normalization
- Source tagging and metadata enrichment

### 3. Analysis & Production
- Semantic clustering of communications
- Thematic classification using AI models (e.g., WMDs, cyber, political unrest)
- Interactive natural language querying
- Source-attributed intelligence output

### 4. Dissemination 
- Export support to CHRONICLE for auto-generated reports (Planned)

---

## System Architecture 

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  OSINT Data Feeds  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Ingestion Pipeline   â”‚
                   â”‚ (Streaming/Batch)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Preprocessing & Enrichment  â”‚
              â”‚ - Language Detection        â”‚
              â”‚ - Translation               â”‚
              â”‚ - Entity Extraction         â”‚
              â”‚ - Deduplication             â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Classification & Clusteringâ”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Vector DB + Metadata Store   â”‚
             â”‚ (FAISS + PostgreSQL/Elastic) â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Natural Language Interface   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚            â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Analyst     â”‚   â”‚ AuthN/AuthZ â”‚
       â”‚ Feedback    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Export to CHRONICLE & Visual UI    â”‚
     â”‚ (Exploration, Graphs, Filtering)   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## Core Modules

- `data_ingestion/`: Streams multilingual OSINT sources via APIs or webhooks
- `preprocessing/`: Handles cleaning, translation, NER, and deduplication
- `classification/`: Identifies content themes using fine-tuned language models
- `clustering/`: Groups related documents by semantic similarity
- `vector_store/`: FAISS-based vector search engine with PostgreSQL/Elasticsearch metadata
- `nl_interface/`: Natural language query handler with retrieval-based output
- `ui/`: Visual exploration dashboard (Streamlit-based)
- `auth/`: Authentication and authorization services
- `integration/chronicle_exporter/`: Optional output pipe to CHRONICLE

---

## Development Roadmap

### **Phase 1 â€“ Collection & Processing MVP**
- [x] Set up GitHub repo + documentation
- [ ] Ingest multilingual OSINT from RSS/news/social APIs
- [ ] Implement preprocessing:
  - Language detection
  - Translation
  - Named Entity Recognition (NER)
  - Deduplication
- [ ] Prototype clustering and classification using sentence embeddings
- [ ] Integrate vector similarity search with FAISS
- [ ] Implement metadata filtering (PostgreSQL or Elasticsearch)

---

### **Phase 2 â€“ Analyst Interface & Intelligence Exploration**
- [ ] Build Streamlit-based visual dashboard
- [ ] Natural language query integration using RAG pipeline (LangChain/OpenAI)
- [ ] Cluster exploration with interactive graph visualization (Plotly or PyVis)
- [ ] Export intelligence to CHRONICLE format (Markdown, PDF)
- [ ] Analyst feedback capture and relevance scoring
- [ ] Implement secure AuthN/AuthZ (role-based access control)

---

### **Phase 3 â€“ Production Readiness & DevOps**
- [ ] Containerize application with Docker
- [ ] Deploy to cloud platform (GCP or AWS)
- [ ] Set up CI/CD pipelines using GitHub Actions
- [ ] Implement basic logging and monitoring (e.g., Streamlit logs, Prometheus optional)
- [ ] Add basic unit and integration tests for core components
- [ ] Optional: Track ML models and experiments using MLflow or DVC

---

## Tech Stack

### **Languages & Frameworks**
- **Python 3.11**
- **JavaScript** (Streamlit/React UI via Streamlit Components)
- **FastAPI** â€“ backend microservices and API layer
- **Streamlit** â€“ rapid UI prototyping and dashboard
- **Docker** â€“ containerization
- **GitHub Actions** â€“ CI/CD

### **NLP & Machine Learning**
- **spaCy**, **Stanza**, **NLTK** â€“ language preprocessing and NER
- **HuggingFace Transformers** â€“ transformer-based models
- **SentenceTransformers (SBERT)** â€“ document embeddings for clustering & similarity
- **LangChain + OpenAI API** â€“ RAG (retrieval-augmented generation) for natural language interface

### **Data Stores**
- **FAISS** â€“ vector similarity search
- **PostgreSQL** or **Elasticsearch** â€“ metadata indexing and filtering
- **Optional**: **MLflow** â€“ model versioning and experiment tracking

### **Visualization**
- **Plotly**, **PyVis**, **NetworkX** â€“ interactive cluster and entity graphing

### **Deployment**
- **GCP** or **AWS** (planned cloud host)
- **Prometheus/Grafana** (optional monitoring)


---

## Folder Structure 

## ğŸ“ Folder Structure 

- el-roi/
  - auth/
    - login.py
    - roles_config.yaml
  - classification/
    - topic_classifier.py
    - embeddings.py
  - clustering/
    - cluster_engine.py
  - config/
    - pipeline_config.yaml
    - model_params.yaml
    - sources_config.yaml
  - data_ingestion/
    - feeds/
    - scheduler.py
  - deployment/
    - Dockerfile
    - cloud_setup/
    - github_actions/
  - docs/
    - architecture.md
    - api_reference.md
  - feedback/
    - feedback_logger.py
  - integration/
    - chronicle_exporter/
      - exporter.py
  - logging/
    - logger.py
    - log_config.yaml
  - models/
    - embedding_model.py
    - classifier_model.py
    - model_utils.py
  - nl_interface/
    - query_handler.py
    - rag_pipeline.py
  - preprocessing/
    - pipeline.py
    - language_detection.py
    - translation.py
    - ner.py
    - deduplication.py
  - tests/
    - test_preprocessing.py
    - test_vector_store.py
    - test_query_handler.py
  - ui/
    - pages/
    - components/
  - vector_store/
    - faiss_store.py
    - metadata_index.py
  - visualization/
    - entity_graph.py
    - cluster_map.py
  - .env.example
  - .gitignore
  - requirements.txt
  - README.md
  - .secrets/
    - openai_api.key
    - gcp_creds.json


---

## Getting Started (via GitHub GUI)

1. Clone or download this repository from GitHub.
2. Open `.env.example` and fill in required API keys and credentials.
3. Use Streamlit locally to test interface modules (`ui/`).
4. Use the GitHub Projects tab to track development progress.

---

## Usage

This section will be expanded as features are built. Planned capabilities include:

- Start ingestion of multilingual OSINT feeds
- Run automated enrichment (NER, deduplication, etc.)
- Classify and cluster documents by topic
- Search using natural language questions
- Export filtered content to CHRONICLE format
- Visualize results and entity relationships via dashboard

---

## Contributing

Contributions, feedback, and ideas are welcome! Please:

- Fork the repository
- Create a feature branch (`feature-name`)
- Commit your changes
- Submit a Pull Request

For strategic features or integrations, open an issue for discussion.

---

## License

All rights reserved. This project is protected by copyright.

You may not copy, distribute, modify, or use any part of this project without explicit written permission from the author.

---

## Contact

Developed and maintained by **Justin Ryals**  
ğŸ“« [LinkedIn](https://linkedin.com/in/justinryals)  
ğŸ“§ For usage or licensing inquiries, please contact the author directly.


